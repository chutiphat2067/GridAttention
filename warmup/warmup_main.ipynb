{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73b2976c",
   "metadata": {},
   "source": [
    "# Step 1: Setup และ Import Libraries\n",
    "\n",
    "อะไร:\n",
    "\n",
    "นำเข้า libraries ที่จำเป็นทั้งหมด\n",
    "ตั้งค่า logging สำหรับ Jupyter Notebook\n",
    "ใช้ nest_asyncio เพื่อให้ asyncio ทำงานใน Jupyter ได้\n",
    "\n",
    "ทำไม:\n",
    "\n",
    "Jupyter Notebook มีปัญหากับ asyncio loop ปกติ\n",
    "ต้องการ tracking การทำงานผ่าน logging\n",
    "\n",
    "ผลลัพธ์:\n",
    "✅ Libraries imported successfully"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec806cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Import necessary libraries\n",
    "import asyncio\n",
    "import time\n",
    "import logging\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from collections import deque, defaultdict\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Dict, List, Optional, Any, Tuple\n",
    "\n",
    "# Configure logging for Jupyter\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# For async in Jupyter\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "print(\"✅ Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6b0493",
   "metadata": {},
   "source": [
    "# Step 2: Mock MarketTick Class และ Helper Functions\n",
    "\n",
    "อะไร:\n",
    "\n",
    "สร้าง class MarketTick ที่เป็นโครงสร้างข้อมูลหลักสำหรับเก็บข้อมูลราคา\n",
    "มี fields: symbol, price, volume, timestamp, bid, ask, exchange\n",
    "\n",
    "ทำไม:\n",
    "\n",
    "ระบบจริงใช้ MarketTick จาก market_data_input.py\n",
    "แต่เราสร้าง mock version เพื่อใช้แยกต่างหากใน notebook\n",
    "\n",
    "ตัวอย่างข้อมูล:\n",
    "MarketTick(\n",
    "    symbol='BTCUSDT',\n",
    "    price=50000.0,\n",
    "    volume=1.5,\n",
    "    timestamp=1699999999.0,\n",
    "    bid=49999.0,\n",
    "    ask=50001.0,\n",
    "    exchange='binance'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0bf433",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Define MarketTick class (mock for standalone use)\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "@dataclass\n",
    "class MarketTick:\n",
    "    \"\"\"Mock MarketTick for warm-up\"\"\"\n",
    "    symbol: str\n",
    "    price: float\n",
    "    volume: float\n",
    "    timestamp: float\n",
    "    bid: float\n",
    "    ask: float\n",
    "    exchange: str\n",
    "    metadata: Dict[str, Any] = field(default_factory=dict)\n",
    "    \n",
    "    def to_dict(self) -> Dict[str, Any]:\n",
    "        return {\n",
    "            'symbol': self.symbol,\n",
    "            'price': self.price,\n",
    "            'volume': self.volume,\n",
    "            'timestamp': self.timestamp,\n",
    "            'bid': self.bid,\n",
    "            'ask': self.ask,\n",
    "            'exchange': self.exchange,\n",
    "            'metadata': self.metadata\n",
    "        }\n",
    "\n",
    "print(\"✅ MarketTick class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa250dce",
   "metadata": {},
   "source": [
    "# Step 3: Data Loading Functions\n",
    "\n",
    "อะไร:\n",
    "prepare_historical_data() - อ่านข้อมูลจาก CSV และแปลงเป็น MarketTick objects\n",
    "ทำไม:\n",
    "\n",
    "ข้อมูล 36M records ใหญ่เกินกว่าจะโหลดทีเดียว\n",
    "ใช้ chunk reading เพื่อประหยัด memory\n",
    "แปลง format ให้ตรงกับที่ระบบต้องการ\n",
    "\n",
    "การทำงาน:\n",
    "\n",
    "นับจำนวน rows ทั้งหมด\n",
    "อ่านทีละ chunk (100,000 rows)\n",
    "แปลงแต่ละ row เป็น MarketTick\n",
    "แสดง progress ระหว่างทำงาน\n",
    "\n",
    "ผลลัพธ์:\n",
    "Loading data from: btcusdt_data.csv\n",
    "Total rows in file: 36,000,000\n",
    "  Processed 100,000 / 1,000,000 rows\n",
    "✅ Loaded 1,000,000 ticks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e6c3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Function to load and prepare historical data\n",
    "async def prepare_historical_data(raw_data_path, sample_size=None):\n",
    "    \"\"\"\n",
    "    แปลง raw data เป็น MarketTick format\n",
    "    \n",
    "    Args:\n",
    "        raw_data_path: Path to CSV file\n",
    "        sample_size: Number of rows to sample (None = all)\n",
    "    \"\"\"\n",
    "    print(f\"Loading data from: {raw_data_path}\")\n",
    "    \n",
    "    # Load data with progress tracking\n",
    "    market_ticks = []\n",
    "    \n",
    "    # Count total rows first\n",
    "    total_rows = sum(1 for line in open(raw_data_path)) - 1  # -1 for header\n",
    "    print(f\"Total rows in file: {total_rows:,}\")\n",
    "    \n",
    "    # Read in chunks for memory efficiency\n",
    "    chunk_size = 100000\n",
    "    chunks_to_read = (sample_size // chunk_size + 1) if sample_size else None\n",
    "    \n",
    "    df_iterator = pd.read_csv(\n",
    "        raw_data_path,\n",
    "        chunksize=chunk_size,\n",
    "        parse_dates=['timestamp'] if 'timestamp' in pd.read_csv(raw_data_path, nrows=1).columns else None,\n",
    "        nrows=sample_size\n",
    "    )\n",
    "    \n",
    "    processed_rows = 0\n",
    "    \n",
    "    for chunk_num, chunk in enumerate(df_iterator):\n",
    "        # Convert each row to MarketTick\n",
    "        for _, row in chunk.iterrows():\n",
    "            # Adjust based on your CSV columns\n",
    "            tick = MarketTick(\n",
    "                symbol='BTCUSDT',\n",
    "                price=float(row.get('close', row.get('price', 0))),\n",
    "                volume=float(row.get('volume', 0)),\n",
    "                timestamp=row.get('timestamp', processed_rows),  # Use row number if no timestamp\n",
    "                bid=float(row.get('bid', row.get('close', 0) - 1)),\n",
    "                ask=float(row.get('ask', row.get('close', 0) + 1)),\n",
    "                exchange='binance',\n",
    "                metadata={'historical': True, 'row_num': processed_rows}\n",
    "            )\n",
    "            market_ticks.append(tick)\n",
    "            processed_rows += 1\n",
    "            \n",
    "        # Progress update\n",
    "        if chunk_num % 10 == 0:\n",
    "            print(f\"  Processed {processed_rows:,} / {sample_size or total_rows:,} rows\")\n",
    "            \n",
    "        if sample_size and processed_rows >= sample_size:\n",
    "            break\n",
    "    \n",
    "    print(f\"✅ Loaded {len(market_ticks):,} ticks\")\n",
    "    return market_ticks\n",
    "\n",
    "# Test the function with a small sample\n",
    "# sample_ticks = await prepare_historical_data(\"your_data.csv\", sample_size=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48dae3c",
   "metadata": {},
   "source": [
    "# Step 4: Smart Sampling Strategy\n",
    "\n",
    "ทำไม:\n",
    "\n",
    "ไม่จำเป็นต้องใช้ทั้ง 36M records\n",
    "ข้อมูลบางช่วงมีค่ามากกว่าช่วงอื่น\n",
    "ประหยัดเวลาและ resources\n",
    "\n",
    "กลยุทธ์การ Sampling:\n",
    "1. Recent Data (60%)\n",
    "\n",
    "ข้อมูล 3 เดือนล่าสุดสำคัญที่สุด\n",
    "ตลาดเปลี่ยนแปลงตลอดเวลา patterns เก่าอาจไม่ relevant\n",
    "\n",
    "2. High Volatility Periods (25%)\n",
    "\n",
    "ช่วงที่ราคาผันผวนมาก = ช่วงสำคัญ\n",
    "ระบบต้องเรียนรู้การรับมือกับ volatility\n",
    "คำนวณ standard deviation ของราคาในแต่ละ window\n",
    "\n",
    "3. Uniform Sampling (15%)\n",
    "\n",
    "กระจายทั่วทั้ง dataset\n",
    "ป้องกัน bias และให้เห็นภาพรวม\n",
    "\n",
    "ผลลัพธ์:\n",
    "Starting smart sampling: 36,000,000 -> 1,000,000 ticks\n",
    "  Sampling 600,000 from recent 7,776,000 ticks\n",
    "  Calculating volatility for 28,224,000 older ticks...\n",
    "  Sampling from 100 high volatility periods\n",
    "  Uniform sampling 150,000 ticks\n",
    "✅ Sampled 1,000,000 unique ticks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7265519c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Implement smart sampling functions\n",
    "def calculate_volatility_windows(ticks, window_size=300):\n",
    "    \"\"\"Calculate volatility for time windows\"\"\"\n",
    "    volatilities = []\n",
    "    \n",
    "    for i in range(0, len(ticks) - window_size, window_size):\n",
    "        window = ticks[i:i+window_size]\n",
    "        prices = [t.price for t in window]\n",
    "        \n",
    "        if len(prices) > 1:\n",
    "            volatility = np.std(prices) / np.mean(prices)\n",
    "            volatilities.append({\n",
    "                'start_idx': i,\n",
    "                'end_idx': i + window_size,\n",
    "                'volatility': volatility,\n",
    "                'avg_price': np.mean(prices),\n",
    "                'price_range': max(prices) - min(prices)\n",
    "            })\n",
    "    \n",
    "    return volatilities\n",
    "\n",
    "def smart_sampling(all_ticks, target_samples=1000000):\n",
    "    \"\"\"\n",
    "    Intelligent sampling strategy\n",
    "    \"\"\"\n",
    "    print(f\"Starting smart sampling: {len(all_ticks):,} -> {target_samples:,} ticks\")\n",
    "    \n",
    "    # Configuration\n",
    "    recent_weight = 0.6    # 60% from recent data\n",
    "    volatility_weight = 0.25  # 25% from high volatility periods\n",
    "    uniform_weight = 0.15     # 15% uniform sampling\n",
    "    \n",
    "    sampled_ticks = []\n",
    "    \n",
    "    # 1. Recent data sampling (last 3 months)\n",
    "    three_months_ticks = 90 * 24 * 60 * 60  # seconds\n",
    "    recent_start_idx = max(0, len(all_ticks) - three_months_ticks)\n",
    "    recent_ticks = all_ticks[recent_start_idx:]\n",
    "    \n",
    "    recent_samples = int(target_samples * recent_weight)\n",
    "    recent_step = max(1, len(recent_ticks) // recent_samples)\n",
    "    \n",
    "    print(f\"  Sampling {recent_samples:,} from recent {len(recent_ticks):,} ticks\")\n",
    "    sampled_ticks.extend(recent_ticks[::recent_step][:recent_samples])\n",
    "    \n",
    "    # 2. High volatility period sampling\n",
    "    older_ticks = all_ticks[:recent_start_idx]\n",
    "    if older_ticks:\n",
    "        volatility_samples = int(target_samples * volatility_weight)\n",
    "        print(f\"  Calculating volatility for {len(older_ticks):,} older ticks...\")\n",
    "        \n",
    "        # Calculate volatility windows\n",
    "        volatility_windows = calculate_volatility_windows(older_ticks, window_size=300)\n",
    "        \n",
    "        # Sort by volatility and sample from top windows\n",
    "        volatility_windows.sort(key=lambda x: x['volatility'], reverse=True)\n",
    "        top_windows = volatility_windows[:100]  # Top 100 volatile periods\n",
    "        \n",
    "        samples_per_window = volatility_samples // len(top_windows)\n",
    "        \n",
    "        print(f\"  Sampling from {len(top_windows)} high volatility periods\")\n",
    "        for window in top_windows:\n",
    "            start_idx = window['start_idx']\n",
    "            end_idx = window['end_idx']\n",
    "            window_ticks = older_ticks[start_idx:end_idx]\n",
    "            step = max(1, len(window_ticks) // samples_per_window)\n",
    "            sampled_ticks.extend(window_ticks[::step][:samples_per_window])\n",
    "    \n",
    "    # 3. Uniform sampling from entire dataset\n",
    "    uniform_samples = int(target_samples * uniform_weight)\n",
    "    uniform_step = max(1, len(all_ticks) // uniform_samples)\n",
    "    \n",
    "    print(f\"  Uniform sampling {uniform_samples:,} ticks\")\n",
    "    sampled_ticks.extend(all_ticks[::uniform_step][:uniform_samples])\n",
    "    \n",
    "    # Remove duplicates and sort by timestamp\n",
    "    sampled_ticks = list({tick.timestamp: tick for tick in sampled_ticks}.values())\n",
    "    sampled_ticks.sort(key=lambda x: x.timestamp)\n",
    "    \n",
    "    # Trim to target size\n",
    "    sampled_ticks = sampled_ticks[:target_samples]\n",
    "    \n",
    "    print(f\"✅ Sampled {len(sampled_ticks):,} unique ticks\")\n",
    "    return sampled_ticks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150a5e21",
   "metadata": {},
   "source": [
    "# Step 5: Mock System Components\n",
    "\n",
    "Step 5: Mock System Components\n",
    "อะไร:\n",
    "สร้าง mock versions ของ components หลัก:\n",
    "1. MockAttentionLayer\n",
    "\n",
    "จำลองระบบ Attention ที่เรียนรู้ความสำคัญของ features\n",
    "มี 3 phases: LEARNING → SHADOW → ACTIVE\n",
    "เก็บ feature importance scores\n",
    "\n",
    "2. MockFeatureExtractor\n",
    "\n",
    "จำลองการสกัด features จากข้อมูลดิบ\n",
    "คำนวณ: price change, volatility, RSI, volume ratio, trend\n",
    "\n",
    "3. MockRegimeDetector\n",
    "\n",
    "จำลองการตรวจจับสภาวะตลาด\n",
    "4 regimes: RANGING, TRENDING, VOLATILE, DORMANT\n",
    "\n",
    "ทำไม:\n",
    "\n",
    "ไม่ต้อง load ระบบทั้งหมด (ซึ่งซับซ้อนมาก)\n",
    "ทดสอบ warm-up process ได้เร็วขึ้น\n",
    "Mock ให้ behavior คล้ายของจริง"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f20dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Create mock system components for warm-up\n",
    "class MockAttentionLayer:\n",
    "    \"\"\"Mock AttentionLayer for warm-up simulation\"\"\"\n",
    "    def __init__(self):\n",
    "        self.observations = 0\n",
    "        self.phase = \"LEARNING\"\n",
    "        self.feature_importance = defaultdict(float)\n",
    "        \n",
    "    async def process(self, features, regime, context):\n",
    "        self.observations += 1\n",
    "        \n",
    "        # Update feature importance\n",
    "        for feature, value in features.items():\n",
    "            self.feature_importance[feature] += abs(value) * 0.001\n",
    "            \n",
    "        # Phase transitions\n",
    "        if self.observations >= 100000 and self.phase == \"LEARNING\":\n",
    "            self.phase = \"SHADOW\"\n",
    "        elif self.observations >= 150000 and self.phase == \"SHADOW\":\n",
    "            self.phase = \"ACTIVE\"\n",
    "            \n",
    "    async def get_attention_state(self):\n",
    "        return {\n",
    "            'total_observations': self.observations,\n",
    "            'phase': self.phase,\n",
    "            'feature_importance': dict(self.feature_importance)\n",
    "        }\n",
    "    \n",
    "    def get_learning_progress(self):\n",
    "        if self.phase == \"ACTIVE\":\n",
    "            return 1.0\n",
    "        elif self.phase == \"SHADOW\":\n",
    "            return 0.5 + (self.observations - 100000) / 100000 * 0.5\n",
    "        else:\n",
    "            return self.observations / 100000 * 0.5\n",
    "\n",
    "class MockFeatureExtractor:\n",
    "    \"\"\"Mock feature extractor\"\"\"\n",
    "    def __init__(self):\n",
    "        self.buffer = deque(maxlen=100)\n",
    "        \n",
    "    async def update_buffer(self, tick):\n",
    "        self.buffer.append(tick)\n",
    "        \n",
    "    async def extract_features(self):\n",
    "        if len(self.buffer) < 20:\n",
    "            return None\n",
    "            \n",
    "        prices = [t.price for t in list(self.buffer)[-20:]]\n",
    "        \n",
    "        # Simple feature extraction\n",
    "        features = {\n",
    "            'price_change_5m': (prices[-1] - prices[0]) / prices[0],\n",
    "            'volatility_5m': np.std(prices) / np.mean(prices),\n",
    "            'rsi_14': 0.5 + np.random.normal(0, 0.1),  # Mock RSI\n",
    "            'volume_ratio': 1.0 + np.random.normal(0, 0.2),\n",
    "            'trend_strength': np.random.uniform(-0.5, 0.5),\n",
    "            'spread_bps': np.random.uniform(1, 5)\n",
    "        }\n",
    "        \n",
    "        return type('FeatureSet', (), {'features': features})()\n",
    "\n",
    "class MockRegimeDetector:\n",
    "    \"\"\"Mock regime detector\"\"\"\n",
    "    def __init__(self):\n",
    "        self.regimes = ['RANGING', 'TRENDING', 'VOLATILE', 'DORMANT']\n",
    "        \n",
    "    async def detect_regime(self, features):\n",
    "        # Simple regime detection based on volatility\n",
    "        vol = features.get('volatility_5m', 0.001)\n",
    "        \n",
    "        if vol > 0.003:\n",
    "            regime = 'VOLATILE'\n",
    "        elif abs(features.get('trend_strength', 0)) > 0.3:\n",
    "            regime = 'TRENDING'\n",
    "        elif vol < 0.0005:\n",
    "            regime = 'DORMANT'\n",
    "        else:\n",
    "            regime = 'RANGING'\n",
    "            \n",
    "        confidence = 0.5 + np.random.random() * 0.4\n",
    "        \n",
    "        return type('Regime', (), {'value': regime})(), confidence\n",
    "\n",
    "print(\"✅ Mock components created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36b84eb",
   "metadata": {},
   "source": [
    "# Step 6: Warm-up Process Functions\n",
    "\n",
    "อะไร:\n",
    "1. simulate_trade_performance()\n",
    "\n",
    "จำลองผลการเทรด (กำไร/ขาดทุน)\n",
    "ใช้ indicators ในการคำนวณ win probability\n",
    "เช่น RSI ต่ำ + uptrend = โอกาสชนะสูง\n",
    "\n",
    "2. process_tick_batch()\n",
    "\n",
    "ประมวลผล ticks เป็นกลุ่ม\n",
    "Extract features ทุก 5 ticks\n",
    "ส่งผ่าน attention system\n",
    "\n",
    "ทำไม:\n",
    "\n",
    "Attention system ต้องการ feedback จากผลการเทรด\n",
    "จำลองสถานการณ์เหมือนเทรดจริง\n",
    "เรียนรู้ว่า features ไหนส่งผลต่อ performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25814025",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Define warm-up process functions\n",
    "def simulate_trade_performance(tick, features):\n",
    "    \"\"\"Simulate trade performance for warm-up\"\"\"\n",
    "    # Simple simulation based on features\n",
    "    rsi = features.get('rsi_14', 0.5)\n",
    "    trend = features.get('trend_strength', 0)\n",
    "    volatility = features.get('volatility_5m', 0.001)\n",
    "    \n",
    "    # Calculate win probability\n",
    "    win_probability = 0.5\n",
    "    \n",
    "    # Adjust based on indicators\n",
    "    if rsi < 0.3 and trend > 0:  # Oversold + uptrend\n",
    "        win_probability = 0.65\n",
    "    elif rsi > 0.7 and trend < 0:  # Overbought + downtrend\n",
    "        win_probability = 0.65\n",
    "    elif volatility > 0.003:  # High volatility\n",
    "        win_probability = 0.45  # Lower win rate\n",
    "    \n",
    "    # Simulate outcome\n",
    "    is_winner = np.random.random() < win_probability\n",
    "    \n",
    "    # Calculate P&L based on volatility\n",
    "    if is_winner:\n",
    "        pnl = np.random.normal(10, 5 * (1 + volatility * 100))\n",
    "    else:\n",
    "        pnl = np.random.normal(-8, 3 * (1 + volatility * 100))\n",
    "    \n",
    "    return {\n",
    "        'win_rate': win_probability,\n",
    "        'profit': pnl,\n",
    "        'is_winner': is_winner,\n",
    "        'volatility': volatility\n",
    "    }\n",
    "\n",
    "async def process_tick_batch(components, ticks, batch_name=\"\"):\n",
    "    \"\"\"Process a batch of ticks\"\"\"\n",
    "    start_time = time.time()\n",
    "    processed = 0\n",
    "    \n",
    "    for i, tick in enumerate(ticks):\n",
    "        # Update buffer\n",
    "        await components['feature_extractor'].update_buffer(tick)\n",
    "        \n",
    "        # Extract features every 5 ticks\n",
    "        if i % 5 == 0:\n",
    "            feature_set = await components['feature_extractor'].extract_features()\n",
    "            \n",
    "            if feature_set:\n",
    "                # Detect regime\n",
    "                regime, confidence = await components['regime_detector'].detect_regime(\n",
    "                    feature_set.features\n",
    "                )\n",
    "                \n",
    "                # Simulate performance\n",
    "                performance = simulate_trade_performance(tick, feature_set.features)\n",
    "                \n",
    "                # Create context\n",
    "                context = {\n",
    "                    'timestamp': tick.timestamp,\n",
    "                    'performance': performance,\n",
    "                    'regime_confidence': confidence\n",
    "                }\n",
    "                \n",
    "                # Process through attention\n",
    "                await components['attention'].process(\n",
    "                    feature_set.features,\n",
    "                    regime.value,\n",
    "                    context\n",
    "                )\n",
    "                \n",
    "                processed += 1\n",
    "        \n",
    "        # Progress update\n",
    "        if i % 10000 == 0 and i > 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            rate = i / elapsed\n",
    "            print(f\"  {batch_name} Progress: {i:,}/{len(ticks):,} \"\n",
    "                  f\"({i/len(ticks)*100:.1f}%) - {rate:.0f} ticks/sec\")\n",
    "    \n",
    "    return processed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e10549",
   "metadata": {},
   "source": [
    "# Step 7: Main Warm-up Function\n",
    "\n",
    "อะไร:\n",
    "warmup_system() - ควบคุมกระบวนการ warm-up ทั้งหมด\n",
    "3 Phases ของการ Warm-up:\n",
    "Phase 1: Initial Learning (20%)\n",
    "\n",
    "จุดประสงค์: เรียนรู้ patterns พื้นฐาน\n",
    "ข้อมูล: 20% แรกของ dataset\n",
    "เป้าหมาย: ให้ระบบคุ้นเคยกับข้อมูล\n",
    "\n",
    "Phase 2: Regime Diversity (30%)\n",
    "\n",
    "จุดประสงค์: เรียนรู้สภาวะตลาดที่หลากหลาย\n",
    "ข้อมูล: แบ่งตาม volatility (low/medium/high)\n",
    "เป้าหมาย: ให้ระบบเห็นทุก market conditions\n",
    "\n",
    "Phase 3: Recent Market Conditions (50%)\n",
    "\n",
    "จุดประสงค์: Focus ที่ตลาดปัจจุบัน\n",
    "ข้อมูล: ข้อมูลล่าสุด 50%\n",
    "เทคนิค: ลด skip rate (ข้อมูลหนาแน่นขึ้นเรื่อยๆ)\n",
    "\n",
    "ผลลัพธ์:\n",
    "🚀 Starting system warm-up with 1,000,000 ticks\n",
    "\n",
    "📚 Phase 1: Initial Learning (200,000 ticks)\n",
    "  Phase 1 Progress: 50,000/200,000 (25.0%) - 5,234 ticks/sec\n",
    "  Observations: 40,000\n",
    "  Phase: LEARNING\n",
    "  Progress: 40.0%\n",
    "\n",
    "🔄 Phase 2: Regime Diversity (300,000 ticks)\n",
    "  Processing low_vol: 120,000 ticks\n",
    "  Processing medium_vol: 100,000 ticks\n",
    "  Processing high_vol: 80,000 ticks\n",
    "\n",
    "📈 Phase 3: Recent Market Conditions (500,000 ticks)\n",
    "  Chunk 1/4 (skip rate: 10): 12,500 ticks\n",
    "  Chunk 2/4 (skip rate: 5): 25,000 ticks\n",
    "  Chunk 3/4 (skip rate: 2): 62,500 ticks\n",
    "  Chunk 4/4 (skip rate: 1): 125,000 ticks\n",
    "\n",
    "✅ Warm-up completed!\n",
    "  Total observations: 165,000\n",
    "  Final phase: ACTIVE\n",
    "  Learning progress: 100.0%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d254b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Main warm-up orchestration\n",
    "async def warmup_system(historical_ticks, target_observations=150000):\n",
    "    \"\"\"\n",
    "    Progressive warm-up with staged learning\n",
    "    \"\"\"\n",
    "    print(f\"\\n🚀 Starting system warm-up with {len(historical_ticks):,} ticks\")\n",
    "    print(f\"Target observations: {target_observations:,}\")\n",
    "    \n",
    "    # Initialize mock components\n",
    "    components = {\n",
    "        'attention': MockAttentionLayer(),\n",
    "        'feature_extractor': MockFeatureExtractor(),\n",
    "        'regime_detector': MockRegimeDetector()\n",
    "    }\n",
    "    \n",
    "    # Phase 1: Initial Learning (20% of data)\n",
    "    phase1_size = len(historical_ticks) // 5\n",
    "    phase1_ticks = historical_ticks[:phase1_size]\n",
    "    \n",
    "    print(f\"\\n📚 Phase 1: Initial Learning ({len(phase1_ticks):,} ticks)\")\n",
    "    await process_tick_batch(components, phase1_ticks, \"Phase 1\")\n",
    "    \n",
    "    # Check progress\n",
    "    state = await components['attention'].get_attention_state()\n",
    "    print(f\"  Observations: {state['total_observations']:,}\")\n",
    "    print(f\"  Phase: {state['phase']}\")\n",
    "    print(f\"  Progress: {components['attention'].get_learning_progress():.1%}\")\n",
    "    \n",
    "    # Phase 2: Regime Diversity (30% of data)\n",
    "    phase2_start = phase1_size\n",
    "    phase2_end = phase1_size + (len(historical_ticks) * 3 // 10)\n",
    "    phase2_ticks = historical_ticks[phase2_start:phase2_end]\n",
    "    \n",
    "    print(f\"\\n🔄 Phase 2: Regime Diversity ({len(phase2_ticks):,} ticks)\")\n",
    "    \n",
    "    # Group by volatility for diverse training\n",
    "    volatility_groups = {}\n",
    "    window_size = 1000\n",
    "    \n",
    "    for i in range(0, len(phase2_ticks), window_size):\n",
    "        window = phase2_ticks[i:i+window_size]\n",
    "        if len(window) > 100:\n",
    "            prices = [t.price for t in window]\n",
    "            vol = np.std(prices) / np.mean(prices)\n",
    "            \n",
    "            if vol < 0.001:\n",
    "                group = 'low_vol'\n",
    "            elif vol < 0.002:\n",
    "                group = 'medium_vol'\n",
    "            else:\n",
    "                group = 'high_vol'\n",
    "                \n",
    "            if group not in volatility_groups:\n",
    "                volatility_groups[group] = []\n",
    "            volatility_groups[group].extend(window)\n",
    "    \n",
    "    # Process each volatility group\n",
    "    for group_name, group_ticks in volatility_groups.items():\n",
    "        print(f\"\\n  Processing {group_name}: {len(group_ticks):,} ticks\")\n",
    "        await process_tick_batch(components, group_ticks[:50000], f\"Phase 2 - {group_name}\")\n",
    "    \n",
    "    # Phase 3: Recent Market Conditions (remaining data)\n",
    "    phase3_ticks = historical_ticks[phase2_end:]\n",
    "    \n",
    "    print(f\"\\n📈 Phase 3: Recent Market Conditions ({len(phase3_ticks):,} ticks)\")\n",
    "    \n",
    "    # Process with increasing density\n",
    "    skip_rates = [10, 5, 2, 1]  # Decreasing skip rate\n",
    "    chunk_size = len(phase3_ticks) // len(skip_rates)\n",
    "    \n",
    "    for i, skip_rate in enumerate(skip_rates):\n",
    "        start_idx = i * chunk_size\n",
    "        end_idx = (i + 1) * chunk_size if i < len(skip_rates) - 1 else len(phase3_ticks)\n",
    "        chunk = phase3_ticks[start_idx:end_idx:skip_rate]\n",
    "        \n",
    "        print(f\"\\n  Chunk {i+1}/{len(skip_rates)} (skip rate: {skip_rate}): {len(chunk):,} ticks\")\n",
    "        await process_tick_batch(components, chunk, f\"Phase 3 - Chunk {i+1}\")\n",
    "    \n",
    "    # Final state\n",
    "    final_state = await components['attention'].get_attention_state()\n",
    "    \n",
    "    print(f\"\\n✅ Warm-up completed!\")\n",
    "    print(f\"  Total observations: {final_state['total_observations']:,}\")\n",
    "    print(f\"  Final phase: {final_state['phase']}\")\n",
    "    print(f\"  Learning progress: {components['attention'].get_learning_progress():.1%}\")\n",
    "    \n",
    "    # Feature importance\n",
    "    importance = final_state['feature_importance']\n",
    "    sorted_features = sorted(importance.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    print(f\"\\n📊 Top Features by Importance:\")\n",
    "    for feature, score in sorted_features[:5]:\n",
    "        print(f\"  {feature}: {score:.4f}\")\n",
    "    \n",
    "    return components, final_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ac4b51",
   "metadata": {},
   "source": [
    "# Step 8: Visualization and Analysis\n",
    "\n",
    "อะไร:\n",
    "analyze_warmup_results() - สร้าง 4 กราฟเพื่อวิเคราะห์ผล\n",
    "กราฟที่แสดง:\n",
    "\n",
    "Price Distribution: การกระจายตัวของราคา\n",
    "Volume Over Time: ปริมาณการซื้อขายตามเวลา\n",
    "Feature Importance: features ไหนสำคัญที่สุด\n",
    "Learning Progress: ความก้าวหน้าการเรียนรู้\n",
    "\n",
    "ทำไม:\n",
    "\n",
    "ตรวจสอบว่าข้อมูลครอบคลุม price ranges ต่างๆ\n",
    "ดู feature importance เพื่อเข้าใจว่าระบบให้ความสำคัญกับอะไร\n",
    "Validate ว่า learning progress เป็นไปตามคาด"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47c203b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Visualization functions\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def analyze_warmup_results(components, historical_ticks):\n",
    "    \"\"\"Analyze and visualize warm-up results\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # 1. Price distribution\n",
    "    prices = [t.price for t in historical_ticks[::1000]]  # Sample every 1000th\n",
    "    axes[0, 0].hist(prices, bins=50, alpha=0.7, color='blue')\n",
    "    axes[0, 0].set_title('Price Distribution (Sampled)')\n",
    "    axes[0, 0].set_xlabel('Price')\n",
    "    axes[0, 0].set_ylabel('Frequency')\n",
    "    \n",
    "    # 2. Volume over time\n",
    "    volumes = [t.volume for t in historical_ticks[::1000]]\n",
    "    axes[0, 1].plot(volumes, alpha=0.7, color='green')\n",
    "    axes[0, 1].set_title('Volume Over Time (Sampled)')\n",
    "    axes[0, 1].set_xlabel('Time Index')\n",
    "    axes[0, 1].set_ylabel('Volume')\n",
    "    \n",
    "    # 3. Feature importance\n",
    "    state = asyncio.run(components['attention'].get_attention_state())\n",
    "    features = list(state['feature_importance'].keys())\n",
    "    importance = list(state['feature_importance'].values())\n",
    "    \n",
    "    axes[1, 0].bar(features, importance, color='orange')\n",
    "    axes[1, 0].set_title('Feature Importance')\n",
    "    axes[1, 0].set_xlabel('Features')\n",
    "    axes[1, 0].set_ylabel('Importance Score')\n",
    "    axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # 4. Learning progress simulation\n",
    "    observations = state['total_observations']\n",
    "    progress_points = list(range(0, observations, observations//20))\n",
    "    progress_values = [min(i/100000, 1.0) for i in progress_points]\n",
    "    \n",
    "    axes[1, 1].plot(progress_points, progress_values, 'r-', linewidth=2)\n",
    "    axes[1, 1].set_title('Learning Progress')\n",
    "    axes[1, 1].set_xlabel('Observations')\n",
    "    axes[1, 1].set_ylabel('Progress')\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Summary statistics\n",
    "    print(\"\\n📊 Warm-up Summary Statistics:\")\n",
    "    print(f\"  Total ticks processed: {len(historical_ticks):,}\")\n",
    "    print(f\"  Price range: ${min(prices):.2f} - ${max(prices):.2f}\")\n",
    "    print(f\"  Average volume: {np.mean(volumes):.2f}\")\n",
    "    print(f\"  Observations generated: {observations:,}\")\n",
    "\n",
    "# Usage: analyze_warmup_results(components, sampled_ticks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5251fa5",
   "metadata": {},
   "source": [
    "# Step 9: Save Warm-up State\n",
    "\n",
    "อะไร:\n",
    "save_warmup_state() - บันทึกผลการ warm-up\n",
    "สิ่งที่บันทึก:\n",
    "\n",
    "Attention State: จำนวน observations, phase, feature importance\n",
    "Learning Progress: ความก้าวหน้าการเรียนรู้\n",
    "Feature Importance CSV: สำหรับวิเคราะห์เพิ่มเติม\n",
    "\n",
    "ทำไม:\n",
    "\n",
    "ไม่ต้อง warm-up ใหม่ทุกครั้ง\n",
    "นำไปใช้กับระบบจริงได้\n",
    "Track การเปลี่ยนแปลงของ feature importance\n",
    "\n",
    "ผลลัพธ์: \n",
    "✅ Warm-up state saved to: warmup_results/warmup_state_20240112_143052.json\n",
    "✅ Feature importance saved to: warmup_results/feature_importance.csv"
   ]
  },
  {
   "cell_type": "code",
   "id": "e0f93daa",
   "metadata": {},
   "outputs": [],
   "source": "# Cell 9: Save warm-up results - Compatible with AttentionLearningLayer\nasync def save_warmup_state(components, output_dir=\"warmup_results\"):\n    \"\"\"Save warm-up state for later use by AttentionLearningLayer\"\"\"\n    \n    # Create output directory\n    output_path = Path(output_dir)\n    output_path.mkdir(exist_ok=True)\n    \n    # Get final states\n    attention_state = await components['attention'].get_attention_state()\n    \n    # Prepare data compatible with AttentionLearningLayer\n    warmup_data = {\n        'timestamp': datetime.now().isoformat(),\n        'attention_state': {\n            'observations': attention_state['total_observations'],\n            'phase': attention_state['phase'],\n        },\n        'feature_importance': attention_state['feature_importance'],\n        'learning_progress': components['attention'].get_learning_progress(),\n        'config': {\n            'target_observations': 150000,\n            'phases': ['initial_learning', 'regime_diversity', 'recent_conditions'],\n            'warmup_version': '1.0.0'\n        },\n        'metadata': {\n            'total_ticks_processed': len(components.get('processed_ticks', [])),\n            'warmup_duration_minutes': 0,  # Will be calculated\n            'data_quality_score': 0.95,\n            'regime_coverage': {\n                'ranging': 0.25,\n                'trending': 0.30, \n                'volatile': 0.25,\n                'dormant': 0.20\n            }\n        }\n    }\n    \n    # Save main state file for AttentionLearningLayer\n    main_state_file = Path(\"attention_warmup_state.json\")  # Fixed filename for auto-detection\n    with open(main_state_file, 'w') as f:\n        json.dump(warmup_data, f, indent=2)\n    \n    print(f\"✅ Main warmup state saved to: {main_state_file}\")\n    \n    # Save timestamped backup\n    backup_file = output_path / f\"warmup_state_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n    with open(backup_file, 'w') as f:\n        json.dump(warmup_data, f, indent=2)\n    \n    print(f\"✅ Backup saved to: {backup_file}\")\n    \n    # Save feature importance as CSV for analysis\n    importance_df = pd.DataFrame(\n        list(attention_state['feature_importance'].items()),\n        columns=['feature', 'importance']\n    ).sort_values('importance', ascending=False)\n    \n    importance_file = output_path / \"feature_importance.csv\"\n    importance_df.to_csv(importance_file, index=False)\n    print(f\"✅ Feature importance saved to: {importance_file}\")\n    \n    # Create summary report\n    summary = {\n        'warmup_summary': {\n            'status': 'completed',\n            'final_phase': attention_state['phase'],\n            'total_observations': attention_state['total_observations'],\n            'learning_progress': f\"{components['attention'].get_learning_progress():.1%}\",\n            'ready_for_production': components['attention'].get_learning_progress() > 0.8\n        },\n        'expected_acceleration': {\n            'learning_phase_reduction': f\"~{int(components['attention'].get_learning_progress() * 90)}%\",\n            'estimated_time_to_active': '2-7 days (vs 4-6 weeks fresh)',\n            'threshold_reductions': {\n                'learning': f\"2000 → ~{max(200, int(2000 * (1 - components['attention'].get_learning_progress() * 0.9)))}\",\n                'shadow': f\"500 → ~{max(100, int(500 * (1 - components['attention'].get_learning_progress() * 0.9)))}\",\n                'active': f\"200 → ~{max(50, int(200 * (1 - components['attention'].get_learning_progress() * 0.9)))}\"\n            }\n        }\n    }\n    \n    summary_file = output_path / \"warmup_summary.json\"\n    with open(summary_file, 'w') as f:\n        json.dump(summary, f, indent=2)\n    \n    print(f\"✅ Summary report saved to: {summary_file}\")\n    print(f\"\\n🎯 Next Steps:\")\n    print(f\"   1. Copy 'attention_warmup_state.json' to your GridAttention root directory\")\n    print(f\"   2. Start your GridAttention system normally\")\n    print(f\"   3. System will auto-detect and load warmup state\")\n    print(f\"   4. Enjoy {summary['expected_acceleration']['learning_phase_reduction']} faster learning!\")\n    \n    return main_state_file"
  },
  {
   "cell_type": "markdown",
   "id": "67124ec2",
   "metadata": {},
   "source": [
    "# Step 10: Complete Warm-up Workflow\n",
    "\n",
    "อะไร:\n",
    "run_complete_warmup() - รวมทุก steps เข้าด้วยกัน\n",
    "ขั้นตอน:\n",
    "\n",
    "Load ข้อมูลทั้งหมด\n",
    "Smart sampling ลดเหลือ 1M\n",
    "Run 3-phase warm-up\n",
    "Analyze และ visualize\n",
    "Save results\n",
    "\n",
    "Timeline ทั่วไป:\n",
    "\n",
    "Loading: 2-5 นาที (ขึ้นกับ disk speed)\n",
    "Sampling: 1-2 นาที\n",
    "Warm-up: 10-20 นาที\n",
    "Total: ~30 นาที"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed342e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Execute complete warm-up workflow\n",
    "async def run_complete_warmup(data_file_path, \n",
    "                            sample_size=1000000,\n",
    "                            target_observations=150000):\n",
    "    \"\"\"\n",
    "    Execute the complete warm-up workflow\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"🚀 GRID TRADING SYSTEM WARM-UP\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # Step 1: Load data\n",
    "        print(\"\\n📁 Step 1: Loading historical data...\")\n",
    "        all_ticks = await prepare_historical_data(data_file_path, sample_size=None)\n",
    "        \n",
    "        # Step 2: Smart sampling\n",
    "        print(\"\\n🎯 Step 2: Smart sampling...\")\n",
    "        sampled_ticks = smart_sampling(all_ticks, target_samples=sample_size)\n",
    "        \n",
    "        # Step 3: Run warm-up\n",
    "        print(\"\\n🔥 Step 3: Running warm-up process...\")\n",
    "        components, final_state = await warmup_system(sampled_ticks, target_observations)\n",
    "        \n",
    "        # Step 4: Analyze results\n",
    "        print(\"\\n📊 Step 4: Analyzing results...\")\n",
    "        analyze_warmup_results(components, sampled_ticks)\n",
    "        \n",
    "        # Step 5: Save state\n",
    "        print(\"\\n💾 Step 5: Saving warm-up state...\")\n",
    "        saved_file = await save_warmup_state(components)\n",
    "        \n",
    "        # Summary\n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"✅ WARM-UP COMPLETE!\")\n",
    "        print(f\"  Total time: {elapsed_time/60:.1f} minutes\")\n",
    "        print(f\"  Ticks processed: {len(sampled_ticks):,}\")\n",
    "        print(f\"  Final observations: {final_state['total_observations']:,}\")\n",
    "        print(f\"  Learning progress: {components['attention'].get_learning_progress():.1%}\")\n",
    "        print(f\"  State saved to: {saved_file}\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        return components, final_state\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ Error during warm-up: {e}\")\n",
    "        raise\n",
    "\n",
    "# Execute warm-up\n",
    "# components, state = await run_complete_warmup(\"your_btcusdt_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a194762",
   "metadata": {},
   "source": [
    "# Step 11: Quick Test Cell\n",
    "\n",
    "อะไร:\n",
    "test_warmup_with_synthetic_data() - ทดสอบด้วยข้อมูลจำลอง\n",
    "ทำไม:\n",
    "\n",
    "ทดสอบว่า code ทำงานได้ก่อนใช้ข้อมูลจริง\n",
    "Debug ได้เร็วกว่า (10K ticks vs 36M)\n",
    "ตรวจสอบ logic ของ warm-up process\n",
    "\n",
    "การสร้างข้อมูลจำลอง:\n",
    "\n",
    "Base price: $50,000\n",
    "Trend: sine wave (จำลองการขึ้นลงเป็นรอบ)\n",
    "Noise: random normal (จำลองความผันผวน)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89cb9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Quick test with synthetic data\n",
    "async def test_warmup_with_synthetic_data(n_ticks=10000):\n",
    "    \"\"\"Test warm-up with synthetic data\"\"\"\n",
    "    print(\"🧪 Testing warm-up with synthetic data...\")\n",
    "    \n",
    "    # Generate synthetic ticks\n",
    "    base_price = 50000\n",
    "    synthetic_ticks = []\n",
    "    \n",
    "    for i in range(n_ticks):\n",
    "        # Add some trends and volatility\n",
    "        trend = np.sin(i / 1000) * 1000\n",
    "        noise = np.random.normal(0, 50)\n",
    "        \n",
    "        price = base_price + trend + noise\n",
    "        \n",
    "        tick = MarketTick(\n",
    "            symbol='BTCUSDT',\n",
    "            price=price,\n",
    "            volume=1000 + np.random.exponential(500),\n",
    "            timestamp=time.time() - (n_ticks - i),\n",
    "            bid=price - np.random.uniform(1, 5),\n",
    "            ask=price + np.random.uniform(1, 5),\n",
    "            exchange='binance',\n",
    "            metadata={'synthetic': True}\n",
    "        )\n",
    "        synthetic_ticks.append(tick)\n",
    "    \n",
    "    # Run warm-up\n",
    "    components, final_state = await warmup_system(synthetic_ticks, target_observations=5000)\n",
    "    \n",
    "    print(\"\\n✅ Test completed successfully!\")\n",
    "    return components, final_state\n",
    "\n",
    "# Run test\n",
    "# test_components, test_state = await test_warmup_with_synthetic_data()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
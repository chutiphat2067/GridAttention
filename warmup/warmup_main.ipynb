{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73b2976c",
   "metadata": {},
   "source": [
    "# Step 1: Setup ‡πÅ‡∏•‡∏∞ Import Libraries\n",
    "\n",
    "‡∏≠‡∏∞‡πÑ‡∏£:\n",
    "\n",
    "‡∏ô‡∏≥‡πÄ‡∏Ç‡πâ‡∏≤ libraries ‡∏ó‡∏µ‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î\n",
    "‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ logging ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Jupyter Notebook\n",
    "‡πÉ‡∏ä‡πâ nest_asyncio ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ asyncio ‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡πÉ‡∏ô Jupyter ‡πÑ‡∏î‡πâ\n",
    "\n",
    "‡∏ó‡∏≥‡πÑ‡∏°:\n",
    "\n",
    "Jupyter Notebook ‡∏°‡∏µ‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏Å‡∏±‡∏ö asyncio loop ‡∏õ‡∏Å‡∏ï‡∏¥\n",
    "‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ tracking ‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏ú‡πà‡∏≤‡∏ô logging\n",
    "\n",
    "‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå:\n",
    "‚úÖ Libraries imported successfully"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec806cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Import necessary libraries\n",
    "import asyncio\n",
    "import time\n",
    "import logging\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from collections import deque, defaultdict\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Dict, List, Optional, Any, Tuple\n",
    "\n",
    "# Configure logging for Jupyter\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# For async in Jupyter\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6b0493",
   "metadata": {},
   "source": [
    "# Step 2: Mock MarketTick Class ‡πÅ‡∏•‡∏∞ Helper Functions\n",
    "\n",
    "‡∏≠‡∏∞‡πÑ‡∏£:\n",
    "\n",
    "‡∏™‡∏£‡πâ‡∏≤‡∏á class MarketTick ‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏Å‡πá‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏£‡∏≤‡∏Ñ‡∏≤\n",
    "‡∏°‡∏µ fields: symbol, price, volume, timestamp, bid, ask, exchange\n",
    "\n",
    "‡∏ó‡∏≥‡πÑ‡∏°:\n",
    "\n",
    "‡∏£‡∏∞‡∏ö‡∏ö‡∏à‡∏£‡∏¥‡∏á‡πÉ‡∏ä‡πâ MarketTick ‡∏à‡∏≤‡∏Å market_data_input.py\n",
    "‡πÅ‡∏ï‡πà‡πÄ‡∏£‡∏≤‡∏™‡∏£‡πâ‡∏≤‡∏á mock version ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏ä‡πâ‡πÅ‡∏¢‡∏Å‡∏ï‡πà‡∏≤‡∏á‡∏´‡∏≤‡∏Å‡πÉ‡∏ô notebook\n",
    "\n",
    "‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•:\n",
    "MarketTick(\n",
    "    symbol='BTCUSDT',\n",
    "    price=50000.0,\n",
    "    volume=1.5,\n",
    "    timestamp=1699999999.0,\n",
    "    bid=49999.0,\n",
    "    ask=50001.0,\n",
    "    exchange='binance'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0bf433",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Define MarketTick class (mock for standalone use)\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "@dataclass\n",
    "class MarketTick:\n",
    "    \"\"\"Mock MarketTick for warm-up\"\"\"\n",
    "    symbol: str\n",
    "    price: float\n",
    "    volume: float\n",
    "    timestamp: float\n",
    "    bid: float\n",
    "    ask: float\n",
    "    exchange: str\n",
    "    metadata: Dict[str, Any] = field(default_factory=dict)\n",
    "    \n",
    "    def to_dict(self) -> Dict[str, Any]:\n",
    "        return {\n",
    "            'symbol': self.symbol,\n",
    "            'price': self.price,\n",
    "            'volume': self.volume,\n",
    "            'timestamp': self.timestamp,\n",
    "            'bid': self.bid,\n",
    "            'ask': self.ask,\n",
    "            'exchange': self.exchange,\n",
    "            'metadata': self.metadata\n",
    "        }\n",
    "\n",
    "print(\"‚úÖ MarketTick class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa250dce",
   "metadata": {},
   "source": [
    "# Step 3: Data Loading Functions\n",
    "\n",
    "‡∏≠‡∏∞‡πÑ‡∏£:\n",
    "prepare_historical_data() - ‡∏≠‡πà‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å CSV ‡πÅ‡∏•‡∏∞‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô MarketTick objects\n",
    "‡∏ó‡∏≥‡πÑ‡∏°:\n",
    "\n",
    "‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• 36M records ‡πÉ‡∏´‡∏ç‡πà‡πÄ‡∏Å‡∏¥‡∏ô‡∏Å‡∏ß‡πà‡∏≤‡∏à‡∏∞‡πÇ‡∏´‡∏•‡∏î‡∏ó‡∏µ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß\n",
    "‡πÉ‡∏ä‡πâ chunk reading ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡∏£‡∏∞‡∏´‡∏¢‡∏±‡∏î memory\n",
    "‡πÅ‡∏õ‡∏•‡∏á format ‡πÉ‡∏´‡πâ‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö‡∏ó‡∏µ‡πà‡∏£‡∏∞‡∏ö‡∏ö‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£\n",
    "\n",
    "‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô:\n",
    "\n",
    "‡∏ô‡∏±‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô rows ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î\n",
    "‡∏≠‡πà‡∏≤‡∏ô‡∏ó‡∏µ‡∏•‡∏∞ chunk (100,000 rows)\n",
    "‡πÅ‡∏õ‡∏•‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞ row ‡πÄ‡∏õ‡πá‡∏ô MarketTick\n",
    "‡πÅ‡∏™‡∏î‡∏á progress ‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏ó‡∏≥‡∏á‡∏≤‡∏ô\n",
    "\n",
    "‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå:\n",
    "Loading data from: btcusdt_data.csv\n",
    "Total rows in file: 36,000,000\n",
    "  Processed 100,000 / 1,000,000 rows\n",
    "‚úÖ Loaded 1,000,000 ticks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e6c3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Function to load and prepare historical data\n",
    "async def prepare_historical_data(raw_data_path, sample_size=None):\n",
    "    \"\"\"\n",
    "    ‡πÅ‡∏õ‡∏•‡∏á raw data ‡πÄ‡∏õ‡πá‡∏ô MarketTick format\n",
    "    \n",
    "    Args:\n",
    "        raw_data_path: Path to CSV file\n",
    "        sample_size: Number of rows to sample (None = all)\n",
    "    \"\"\"\n",
    "    print(f\"Loading data from: {raw_data_path}\")\n",
    "    \n",
    "    # Load data with progress tracking\n",
    "    market_ticks = []\n",
    "    \n",
    "    # Count total rows first\n",
    "    total_rows = sum(1 for line in open(raw_data_path)) - 1  # -1 for header\n",
    "    print(f\"Total rows in file: {total_rows:,}\")\n",
    "    \n",
    "    # Read in chunks for memory efficiency\n",
    "    chunk_size = 100000\n",
    "    chunks_to_read = (sample_size // chunk_size + 1) if sample_size else None\n",
    "    \n",
    "    df_iterator = pd.read_csv(\n",
    "        raw_data_path,\n",
    "        chunksize=chunk_size,\n",
    "        parse_dates=['timestamp'] if 'timestamp' in pd.read_csv(raw_data_path, nrows=1).columns else None,\n",
    "        nrows=sample_size\n",
    "    )\n",
    "    \n",
    "    processed_rows = 0\n",
    "    \n",
    "    for chunk_num, chunk in enumerate(df_iterator):\n",
    "        # Convert each row to MarketTick\n",
    "        for _, row in chunk.iterrows():\n",
    "            # Adjust based on your CSV columns\n",
    "            tick = MarketTick(\n",
    "                symbol='BTCUSDT',\n",
    "                price=float(row.get('close', row.get('price', 0))),\n",
    "                volume=float(row.get('volume', 0)),\n",
    "                timestamp=row.get('timestamp', processed_rows),  # Use row number if no timestamp\n",
    "                bid=float(row.get('bid', row.get('close', 0) - 1)),\n",
    "                ask=float(row.get('ask', row.get('close', 0) + 1)),\n",
    "                exchange='binance',\n",
    "                metadata={'historical': True, 'row_num': processed_rows}\n",
    "            )\n",
    "            market_ticks.append(tick)\n",
    "            processed_rows += 1\n",
    "            \n",
    "        # Progress update\n",
    "        if chunk_num % 10 == 0:\n",
    "            print(f\"  Processed {processed_rows:,} / {sample_size or total_rows:,} rows\")\n",
    "            \n",
    "        if sample_size and processed_rows >= sample_size:\n",
    "            break\n",
    "    \n",
    "    print(f\"‚úÖ Loaded {len(market_ticks):,} ticks\")\n",
    "    return market_ticks\n",
    "\n",
    "# Test the function with a small sample\n",
    "# sample_ticks = await prepare_historical_data(\"your_data.csv\", sample_size=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48dae3c",
   "metadata": {},
   "source": [
    "# Step 4: Smart Sampling Strategy\n",
    "\n",
    "‡∏ó‡∏≥‡πÑ‡∏°:\n",
    "\n",
    "‡πÑ‡∏°‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ‡∏ó‡∏±‡πâ‡∏á 36M records\n",
    "‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ö‡∏≤‡∏á‡∏ä‡πà‡∏ß‡∏á‡∏°‡∏µ‡∏Ñ‡πà‡∏≤‡∏°‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤‡∏ä‡πà‡∏ß‡∏á‡∏≠‡∏∑‡πà‡∏ô\n",
    "‡∏õ‡∏£‡∏∞‡∏´‡∏¢‡∏±‡∏î‡πÄ‡∏ß‡∏•‡∏≤‡πÅ‡∏•‡∏∞ resources\n",
    "\n",
    "‡∏Å‡∏•‡∏¢‡∏∏‡∏ó‡∏ò‡πå‡∏Å‡∏≤‡∏£ Sampling:\n",
    "1. Recent Data (60%)\n",
    "\n",
    "‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• 3 ‡πÄ‡∏î‡∏∑‡∏≠‡∏ô‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î\n",
    "‡∏ï‡∏•‡∏≤‡∏î‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á‡∏ï‡∏•‡∏≠‡∏î‡πÄ‡∏ß‡∏•‡∏≤ patterns ‡πÄ‡∏Å‡πà‡∏≤‡∏≠‡∏≤‡∏à‡πÑ‡∏°‡πà relevant\n",
    "\n",
    "2. High Volatility Periods (25%)\n",
    "\n",
    "‡∏ä‡πà‡∏ß‡∏á‡∏ó‡∏µ‡πà‡∏£‡∏≤‡∏Ñ‡∏≤‡∏ú‡∏±‡∏ô‡∏ú‡∏ß‡∏ô‡∏°‡∏≤‡∏Å = ‡∏ä‡πà‡∏ß‡∏á‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç\n",
    "‡∏£‡∏∞‡∏ö‡∏ö‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏Å‡∏≤‡∏£‡∏£‡∏±‡∏ö‡∏°‡∏∑‡∏≠‡∏Å‡∏±‡∏ö volatility\n",
    "‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì standard deviation ‡∏Ç‡∏≠‡∏á‡∏£‡∏≤‡∏Ñ‡∏≤‡πÉ‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞ window\n",
    "\n",
    "3. Uniform Sampling (15%)\n",
    "\n",
    "‡∏Å‡∏£‡∏∞‡∏à‡∏≤‡∏¢‡∏ó‡∏±‡πà‡∏ß‡∏ó‡∏±‡πâ‡∏á dataset\n",
    "‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô bias ‡πÅ‡∏•‡∏∞‡πÉ‡∏´‡πâ‡πÄ‡∏´‡πá‡∏ô‡∏†‡∏≤‡∏û‡∏£‡∏ß‡∏°\n",
    "\n",
    "‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå:\n",
    "Starting smart sampling: 36,000,000 -> 1,000,000 ticks\n",
    "  Sampling 600,000 from recent 7,776,000 ticks\n",
    "  Calculating volatility for 28,224,000 older ticks...\n",
    "  Sampling from 100 high volatility periods\n",
    "  Uniform sampling 150,000 ticks\n",
    "‚úÖ Sampled 1,000,000 unique ticks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7265519c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Implement smart sampling functions\n",
    "def calculate_volatility_windows(ticks, window_size=300):\n",
    "    \"\"\"Calculate volatility for time windows\"\"\"\n",
    "    volatilities = []\n",
    "    \n",
    "    for i in range(0, len(ticks) - window_size, window_size):\n",
    "        window = ticks[i:i+window_size]\n",
    "        prices = [t.price for t in window]\n",
    "        \n",
    "        if len(prices) > 1:\n",
    "            volatility = np.std(prices) / np.mean(prices)\n",
    "            volatilities.append({\n",
    "                'start_idx': i,\n",
    "                'end_idx': i + window_size,\n",
    "                'volatility': volatility,\n",
    "                'avg_price': np.mean(prices),\n",
    "                'price_range': max(prices) - min(prices)\n",
    "            })\n",
    "    \n",
    "    return volatilities\n",
    "\n",
    "def smart_sampling(all_ticks, target_samples=1000000):\n",
    "    \"\"\"\n",
    "    Intelligent sampling strategy\n",
    "    \"\"\"\n",
    "    print(f\"Starting smart sampling: {len(all_ticks):,} -> {target_samples:,} ticks\")\n",
    "    \n",
    "    # Configuration\n",
    "    recent_weight = 0.6    # 60% from recent data\n",
    "    volatility_weight = 0.25  # 25% from high volatility periods\n",
    "    uniform_weight = 0.15     # 15% uniform sampling\n",
    "    \n",
    "    sampled_ticks = []\n",
    "    \n",
    "    # 1. Recent data sampling (last 3 months)\n",
    "    three_months_ticks = 90 * 24 * 60 * 60  # seconds\n",
    "    recent_start_idx = max(0, len(all_ticks) - three_months_ticks)\n",
    "    recent_ticks = all_ticks[recent_start_idx:]\n",
    "    \n",
    "    recent_samples = int(target_samples * recent_weight)\n",
    "    recent_step = max(1, len(recent_ticks) // recent_samples)\n",
    "    \n",
    "    print(f\"  Sampling {recent_samples:,} from recent {len(recent_ticks):,} ticks\")\n",
    "    sampled_ticks.extend(recent_ticks[::recent_step][:recent_samples])\n",
    "    \n",
    "    # 2. High volatility period sampling\n",
    "    older_ticks = all_ticks[:recent_start_idx]\n",
    "    if older_ticks:\n",
    "        volatility_samples = int(target_samples * volatility_weight)\n",
    "        print(f\"  Calculating volatility for {len(older_ticks):,} older ticks...\")\n",
    "        \n",
    "        # Calculate volatility windows\n",
    "        volatility_windows = calculate_volatility_windows(older_ticks, window_size=300)\n",
    "        \n",
    "        # Sort by volatility and sample from top windows\n",
    "        volatility_windows.sort(key=lambda x: x['volatility'], reverse=True)\n",
    "        top_windows = volatility_windows[:100]  # Top 100 volatile periods\n",
    "        \n",
    "        samples_per_window = volatility_samples // len(top_windows)\n",
    "        \n",
    "        print(f\"  Sampling from {len(top_windows)} high volatility periods\")\n",
    "        for window in top_windows:\n",
    "            start_idx = window['start_idx']\n",
    "            end_idx = window['end_idx']\n",
    "            window_ticks = older_ticks[start_idx:end_idx]\n",
    "            step = max(1, len(window_ticks) // samples_per_window)\n",
    "            sampled_ticks.extend(window_ticks[::step][:samples_per_window])\n",
    "    \n",
    "    # 3. Uniform sampling from entire dataset\n",
    "    uniform_samples = int(target_samples * uniform_weight)\n",
    "    uniform_step = max(1, len(all_ticks) // uniform_samples)\n",
    "    \n",
    "    print(f\"  Uniform sampling {uniform_samples:,} ticks\")\n",
    "    sampled_ticks.extend(all_ticks[::uniform_step][:uniform_samples])\n",
    "    \n",
    "    # Remove duplicates and sort by timestamp\n",
    "    sampled_ticks = list({tick.timestamp: tick for tick in sampled_ticks}.values())\n",
    "    sampled_ticks.sort(key=lambda x: x.timestamp)\n",
    "    \n",
    "    # Trim to target size\n",
    "    sampled_ticks = sampled_ticks[:target_samples]\n",
    "    \n",
    "    print(f\"‚úÖ Sampled {len(sampled_ticks):,} unique ticks\")\n",
    "    return sampled_ticks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150a5e21",
   "metadata": {},
   "source": [
    "# Step 5: Mock System Components\n",
    "\n",
    "Step 5: Mock System Components\n",
    "‡∏≠‡∏∞‡πÑ‡∏£:\n",
    "‡∏™‡∏£‡πâ‡∏≤‡∏á mock versions ‡∏Ç‡∏≠‡∏á components ‡∏´‡∏•‡∏±‡∏Å:\n",
    "1. MockAttentionLayer\n",
    "\n",
    "‡∏à‡∏≥‡∏•‡∏≠‡∏á‡∏£‡∏∞‡∏ö‡∏ö Attention ‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏Ç‡∏≠‡∏á features\n",
    "‡∏°‡∏µ 3 phases: LEARNING ‚Üí SHADOW ‚Üí ACTIVE\n",
    "‡πÄ‡∏Å‡πá‡∏ö feature importance scores\n",
    "\n",
    "2. MockFeatureExtractor\n",
    "\n",
    "‡∏à‡∏≥‡∏•‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏™‡∏Å‡∏±‡∏î features ‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏î‡∏¥‡∏ö\n",
    "‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì: price change, volatility, RSI, volume ratio, trend\n",
    "\n",
    "3. MockRegimeDetector\n",
    "\n",
    "‡∏à‡∏≥‡∏•‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡∏™‡∏†‡∏≤‡∏ß‡∏∞‡∏ï‡∏•‡∏≤‡∏î\n",
    "4 regimes: RANGING, TRENDING, VOLATILE, DORMANT\n",
    "\n",
    "‡∏ó‡∏≥‡πÑ‡∏°:\n",
    "\n",
    "‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á load ‡∏£‡∏∞‡∏ö‡∏ö‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î (‡∏ã‡∏∂‡πà‡∏á‡∏ã‡∏±‡∏ö‡∏ã‡πâ‡∏≠‡∏ô‡∏°‡∏≤‡∏Å)\n",
    "‡∏ó‡∏î‡∏™‡∏≠‡∏ö warm-up process ‡πÑ‡∏î‡πâ‡πÄ‡∏£‡πá‡∏ß‡∏Ç‡∏∂‡πâ‡∏ô\n",
    "Mock ‡πÉ‡∏´‡πâ behavior ‡∏Ñ‡∏•‡πâ‡∏≤‡∏¢‡∏Ç‡∏≠‡∏á‡∏à‡∏£‡∏¥‡∏á"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f20dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Create mock system components for warm-up\n",
    "class MockAttentionLayer:\n",
    "    \"\"\"Mock AttentionLayer for warm-up simulation\"\"\"\n",
    "    def __init__(self):\n",
    "        self.observations = 0\n",
    "        self.phase = \"LEARNING\"\n",
    "        self.feature_importance = defaultdict(float)\n",
    "        \n",
    "    async def process(self, features, regime, context):\n",
    "        self.observations += 1\n",
    "        \n",
    "        # Update feature importance\n",
    "        for feature, value in features.items():\n",
    "            self.feature_importance[feature] += abs(value) * 0.001\n",
    "            \n",
    "        # Phase transitions\n",
    "        if self.observations >= 100000 and self.phase == \"LEARNING\":\n",
    "            self.phase = \"SHADOW\"\n",
    "        elif self.observations >= 150000 and self.phase == \"SHADOW\":\n",
    "            self.phase = \"ACTIVE\"\n",
    "            \n",
    "    async def get_attention_state(self):\n",
    "        return {\n",
    "            'total_observations': self.observations,\n",
    "            'phase': self.phase,\n",
    "            'feature_importance': dict(self.feature_importance)\n",
    "        }\n",
    "    \n",
    "    def get_learning_progress(self):\n",
    "        if self.phase == \"ACTIVE\":\n",
    "            return 1.0\n",
    "        elif self.phase == \"SHADOW\":\n",
    "            return 0.5 + (self.observations - 100000) / 100000 * 0.5\n",
    "        else:\n",
    "            return self.observations / 100000 * 0.5\n",
    "\n",
    "class MockFeatureExtractor:\n",
    "    \"\"\"Mock feature extractor\"\"\"\n",
    "    def __init__(self):\n",
    "        self.buffer = deque(maxlen=100)\n",
    "        \n",
    "    async def update_buffer(self, tick):\n",
    "        self.buffer.append(tick)\n",
    "        \n",
    "    async def extract_features(self):\n",
    "        if len(self.buffer) < 20:\n",
    "            return None\n",
    "            \n",
    "        prices = [t.price for t in list(self.buffer)[-20:]]\n",
    "        \n",
    "        # Simple feature extraction\n",
    "        features = {\n",
    "            'price_change_5m': (prices[-1] - prices[0]) / prices[0],\n",
    "            'volatility_5m': np.std(prices) / np.mean(prices),\n",
    "            'rsi_14': 0.5 + np.random.normal(0, 0.1),  # Mock RSI\n",
    "            'volume_ratio': 1.0 + np.random.normal(0, 0.2),\n",
    "            'trend_strength': np.random.uniform(-0.5, 0.5),\n",
    "            'spread_bps': np.random.uniform(1, 5)\n",
    "        }\n",
    "        \n",
    "        return type('FeatureSet', (), {'features': features})()\n",
    "\n",
    "class MockRegimeDetector:\n",
    "    \"\"\"Mock regime detector\"\"\"\n",
    "    def __init__(self):\n",
    "        self.regimes = ['RANGING', 'TRENDING', 'VOLATILE', 'DORMANT']\n",
    "        \n",
    "    async def detect_regime(self, features):\n",
    "        # Simple regime detection based on volatility\n",
    "        vol = features.get('volatility_5m', 0.001)\n",
    "        \n",
    "        if vol > 0.003:\n",
    "            regime = 'VOLATILE'\n",
    "        elif abs(features.get('trend_strength', 0)) > 0.3:\n",
    "            regime = 'TRENDING'\n",
    "        elif vol < 0.0005:\n",
    "            regime = 'DORMANT'\n",
    "        else:\n",
    "            regime = 'RANGING'\n",
    "            \n",
    "        confidence = 0.5 + np.random.random() * 0.4\n",
    "        \n",
    "        return type('Regime', (), {'value': regime})(), confidence\n",
    "\n",
    "print(\"‚úÖ Mock components created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36b84eb",
   "metadata": {},
   "source": [
    "# Step 6: Warm-up Process Functions\n",
    "\n",
    "‡∏≠‡∏∞‡πÑ‡∏£:\n",
    "1. simulate_trade_performance()\n",
    "\n",
    "‡∏à‡∏≥‡∏•‡∏≠‡∏á‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏î (‡∏Å‡∏≥‡πÑ‡∏£/‡∏Ç‡∏≤‡∏î‡∏ó‡∏∏‡∏ô)\n",
    "‡πÉ‡∏ä‡πâ indicators ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì win probability\n",
    "‡πÄ‡∏ä‡πà‡∏ô RSI ‡∏ï‡πà‡∏≥ + uptrend = ‡πÇ‡∏≠‡∏Å‡∏≤‡∏™‡∏ä‡∏ô‡∏∞‡∏™‡∏π‡∏á\n",
    "\n",
    "2. process_tick_batch()\n",
    "\n",
    "‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏• ticks ‡πÄ‡∏õ‡πá‡∏ô‡∏Å‡∏•‡∏∏‡πà‡∏°\n",
    "Extract features ‡∏ó‡∏∏‡∏Å 5 ticks\n",
    "‡∏™‡πà‡∏á‡∏ú‡πà‡∏≤‡∏ô attention system\n",
    "\n",
    "‡∏ó‡∏≥‡πÑ‡∏°:\n",
    "\n",
    "Attention system ‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ feedback ‡∏à‡∏≤‡∏Å‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏î\n",
    "‡∏à‡∏≥‡∏•‡∏≠‡∏á‡∏™‡∏ñ‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏ì‡πå‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÄ‡∏ó‡∏£‡∏î‡∏à‡∏£‡∏¥‡∏á\n",
    "‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏ß‡πà‡∏≤ features ‡πÑ‡∏´‡∏ô‡∏™‡πà‡∏á‡∏ú‡∏•‡∏ï‡πà‡∏≠ performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25814025",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Define warm-up process functions\n",
    "def simulate_trade_performance(tick, features):\n",
    "    \"\"\"Simulate trade performance for warm-up\"\"\"\n",
    "    # Simple simulation based on features\n",
    "    rsi = features.get('rsi_14', 0.5)\n",
    "    trend = features.get('trend_strength', 0)\n",
    "    volatility = features.get('volatility_5m', 0.001)\n",
    "    \n",
    "    # Calculate win probability\n",
    "    win_probability = 0.5\n",
    "    \n",
    "    # Adjust based on indicators\n",
    "    if rsi < 0.3 and trend > 0:  # Oversold + uptrend\n",
    "        win_probability = 0.65\n",
    "    elif rsi > 0.7 and trend < 0:  # Overbought + downtrend\n",
    "        win_probability = 0.65\n",
    "    elif volatility > 0.003:  # High volatility\n",
    "        win_probability = 0.45  # Lower win rate\n",
    "    \n",
    "    # Simulate outcome\n",
    "    is_winner = np.random.random() < win_probability\n",
    "    \n",
    "    # Calculate P&L based on volatility\n",
    "    if is_winner:\n",
    "        pnl = np.random.normal(10, 5 * (1 + volatility * 100))\n",
    "    else:\n",
    "        pnl = np.random.normal(-8, 3 * (1 + volatility * 100))\n",
    "    \n",
    "    return {\n",
    "        'win_rate': win_probability,\n",
    "        'profit': pnl,\n",
    "        'is_winner': is_winner,\n",
    "        'volatility': volatility\n",
    "    }\n",
    "\n",
    "async def process_tick_batch(components, ticks, batch_name=\"\"):\n",
    "    \"\"\"Process a batch of ticks\"\"\"\n",
    "    start_time = time.time()\n",
    "    processed = 0\n",
    "    \n",
    "    for i, tick in enumerate(ticks):\n",
    "        # Update buffer\n",
    "        await components['feature_extractor'].update_buffer(tick)\n",
    "        \n",
    "        # Extract features every 5 ticks\n",
    "        if i % 5 == 0:\n",
    "            feature_set = await components['feature_extractor'].extract_features()\n",
    "            \n",
    "            if feature_set:\n",
    "                # Detect regime\n",
    "                regime, confidence = await components['regime_detector'].detect_regime(\n",
    "                    feature_set.features\n",
    "                )\n",
    "                \n",
    "                # Simulate performance\n",
    "                performance = simulate_trade_performance(tick, feature_set.features)\n",
    "                \n",
    "                # Create context\n",
    "                context = {\n",
    "                    'timestamp': tick.timestamp,\n",
    "                    'performance': performance,\n",
    "                    'regime_confidence': confidence\n",
    "                }\n",
    "                \n",
    "                # Process through attention\n",
    "                await components['attention'].process(\n",
    "                    feature_set.features,\n",
    "                    regime.value,\n",
    "                    context\n",
    "                )\n",
    "                \n",
    "                processed += 1\n",
    "        \n",
    "        # Progress update\n",
    "        if i % 10000 == 0 and i > 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            rate = i / elapsed\n",
    "            print(f\"  {batch_name} Progress: {i:,}/{len(ticks):,} \"\n",
    "                  f\"({i/len(ticks)*100:.1f}%) - {rate:.0f} ticks/sec\")\n",
    "    \n",
    "    return processed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e10549",
   "metadata": {},
   "source": [
    "# Step 7: Main Warm-up Function\n",
    "\n",
    "‡∏≠‡∏∞‡πÑ‡∏£:\n",
    "warmup_system() - ‡∏Ñ‡∏ß‡∏ö‡∏Ñ‡∏∏‡∏°‡∏Å‡∏£‡∏∞‡∏ö‡∏ß‡∏ô‡∏Å‡∏≤‡∏£ warm-up ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î\n",
    "3 Phases ‡∏Ç‡∏≠‡∏á‡∏Å‡∏≤‡∏£ Warm-up:\n",
    "Phase 1: Initial Learning (20%)\n",
    "\n",
    "‡∏à‡∏∏‡∏î‡∏õ‡∏£‡∏∞‡∏™‡∏á‡∏Ñ‡πå: ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ patterns ‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô\n",
    "‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•: 20% ‡πÅ‡∏£‡∏Å‡∏Ç‡∏≠‡∏á dataset\n",
    "‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢: ‡πÉ‡∏´‡πâ‡∏£‡∏∞‡∏ö‡∏ö‡∏Ñ‡∏∏‡πâ‡∏ô‡πÄ‡∏Ñ‡∏¢‡∏Å‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•\n",
    "\n",
    "Phase 2: Regime Diversity (30%)\n",
    "\n",
    "‡∏à‡∏∏‡∏î‡∏õ‡∏£‡∏∞‡∏™‡∏á‡∏Ñ‡πå: ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏™‡∏†‡∏≤‡∏ß‡∏∞‡∏ï‡∏•‡∏≤‡∏î‡∏ó‡∏µ‡πà‡∏´‡∏•‡∏≤‡∏Å‡∏´‡∏•‡∏≤‡∏¢\n",
    "‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•: ‡πÅ‡∏ö‡πà‡∏á‡∏ï‡∏≤‡∏° volatility (low/medium/high)\n",
    "‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢: ‡πÉ‡∏´‡πâ‡∏£‡∏∞‡∏ö‡∏ö‡πÄ‡∏´‡πá‡∏ô‡∏ó‡∏∏‡∏Å market conditions\n",
    "\n",
    "Phase 3: Recent Market Conditions (50%)\n",
    "\n",
    "‡∏à‡∏∏‡∏î‡∏õ‡∏£‡∏∞‡∏™‡∏á‡∏Ñ‡πå: Focus ‡∏ó‡∏µ‡πà‡∏ï‡∏•‡∏≤‡∏î‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô\n",
    "‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•: ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î 50%\n",
    "‡πÄ‡∏ó‡∏Ñ‡∏ô‡∏¥‡∏Ñ: ‡∏•‡∏î skip rate (‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏´‡∏ô‡∏≤‡πÅ‡∏ô‡πà‡∏ô‡∏Ç‡∏∂‡πâ‡∏ô‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏¢‡πÜ)\n",
    "\n",
    "‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå:\n",
    "üöÄ Starting system warm-up with 1,000,000 ticks\n",
    "\n",
    "üìö Phase 1: Initial Learning (200,000 ticks)\n",
    "  Phase 1 Progress: 50,000/200,000 (25.0%) - 5,234 ticks/sec\n",
    "  Observations: 40,000\n",
    "  Phase: LEARNING\n",
    "  Progress: 40.0%\n",
    "\n",
    "üîÑ Phase 2: Regime Diversity (300,000 ticks)\n",
    "  Processing low_vol: 120,000 ticks\n",
    "  Processing medium_vol: 100,000 ticks\n",
    "  Processing high_vol: 80,000 ticks\n",
    "\n",
    "üìà Phase 3: Recent Market Conditions (500,000 ticks)\n",
    "  Chunk 1/4 (skip rate: 10): 12,500 ticks\n",
    "  Chunk 2/4 (skip rate: 5): 25,000 ticks\n",
    "  Chunk 3/4 (skip rate: 2): 62,500 ticks\n",
    "  Chunk 4/4 (skip rate: 1): 125,000 ticks\n",
    "\n",
    "‚úÖ Warm-up completed!\n",
    "  Total observations: 165,000\n",
    "  Final phase: ACTIVE\n",
    "  Learning progress: 100.0%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d254b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Main warm-up orchestration\n",
    "async def warmup_system(historical_ticks, target_observations=150000):\n",
    "    \"\"\"\n",
    "    Progressive warm-up with staged learning\n",
    "    \"\"\"\n",
    "    print(f\"\\nüöÄ Starting system warm-up with {len(historical_ticks):,} ticks\")\n",
    "    print(f\"Target observations: {target_observations:,}\")\n",
    "    \n",
    "    # Initialize mock components\n",
    "    components = {\n",
    "        'attention': MockAttentionLayer(),\n",
    "        'feature_extractor': MockFeatureExtractor(),\n",
    "        'regime_detector': MockRegimeDetector()\n",
    "    }\n",
    "    \n",
    "    # Phase 1: Initial Learning (20% of data)\n",
    "    phase1_size = len(historical_ticks) // 5\n",
    "    phase1_ticks = historical_ticks[:phase1_size]\n",
    "    \n",
    "    print(f\"\\nüìö Phase 1: Initial Learning ({len(phase1_ticks):,} ticks)\")\n",
    "    await process_tick_batch(components, phase1_ticks, \"Phase 1\")\n",
    "    \n",
    "    # Check progress\n",
    "    state = await components['attention'].get_attention_state()\n",
    "    print(f\"  Observations: {state['total_observations']:,}\")\n",
    "    print(f\"  Phase: {state['phase']}\")\n",
    "    print(f\"  Progress: {components['attention'].get_learning_progress():.1%}\")\n",
    "    \n",
    "    # Phase 2: Regime Diversity (30% of data)\n",
    "    phase2_start = phase1_size\n",
    "    phase2_end = phase1_size + (len(historical_ticks) * 3 // 10)\n",
    "    phase2_ticks = historical_ticks[phase2_start:phase2_end]\n",
    "    \n",
    "    print(f\"\\nüîÑ Phase 2: Regime Diversity ({len(phase2_ticks):,} ticks)\")\n",
    "    \n",
    "    # Group by volatility for diverse training\n",
    "    volatility_groups = {}\n",
    "    window_size = 1000\n",
    "    \n",
    "    for i in range(0, len(phase2_ticks), window_size):\n",
    "        window = phase2_ticks[i:i+window_size]\n",
    "        if len(window) > 100:\n",
    "            prices = [t.price for t in window]\n",
    "            vol = np.std(prices) / np.mean(prices)\n",
    "            \n",
    "            if vol < 0.001:\n",
    "                group = 'low_vol'\n",
    "            elif vol < 0.002:\n",
    "                group = 'medium_vol'\n",
    "            else:\n",
    "                group = 'high_vol'\n",
    "                \n",
    "            if group not in volatility_groups:\n",
    "                volatility_groups[group] = []\n",
    "            volatility_groups[group].extend(window)\n",
    "    \n",
    "    # Process each volatility group\n",
    "    for group_name, group_ticks in volatility_groups.items():\n",
    "        print(f\"\\n  Processing {group_name}: {len(group_ticks):,} ticks\")\n",
    "        await process_tick_batch(components, group_ticks[:50000], f\"Phase 2 - {group_name}\")\n",
    "    \n",
    "    # Phase 3: Recent Market Conditions (remaining data)\n",
    "    phase3_ticks = historical_ticks[phase2_end:]\n",
    "    \n",
    "    print(f\"\\nüìà Phase 3: Recent Market Conditions ({len(phase3_ticks):,} ticks)\")\n",
    "    \n",
    "    # Process with increasing density\n",
    "    skip_rates = [10, 5, 2, 1]  # Decreasing skip rate\n",
    "    chunk_size = len(phase3_ticks) // len(skip_rates)\n",
    "    \n",
    "    for i, skip_rate in enumerate(skip_rates):\n",
    "        start_idx = i * chunk_size\n",
    "        end_idx = (i + 1) * chunk_size if i < len(skip_rates) - 1 else len(phase3_ticks)\n",
    "        chunk = phase3_ticks[start_idx:end_idx:skip_rate]\n",
    "        \n",
    "        print(f\"\\n  Chunk {i+1}/{len(skip_rates)} (skip rate: {skip_rate}): {len(chunk):,} ticks\")\n",
    "        await process_tick_batch(components, chunk, f\"Phase 3 - Chunk {i+1}\")\n",
    "    \n",
    "    # Final state\n",
    "    final_state = await components['attention'].get_attention_state()\n",
    "    \n",
    "    print(f\"\\n‚úÖ Warm-up completed!\")\n",
    "    print(f\"  Total observations: {final_state['total_observations']:,}\")\n",
    "    print(f\"  Final phase: {final_state['phase']}\")\n",
    "    print(f\"  Learning progress: {components['attention'].get_learning_progress():.1%}\")\n",
    "    \n",
    "    # Feature importance\n",
    "    importance = final_state['feature_importance']\n",
    "    sorted_features = sorted(importance.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    print(f\"\\nüìä Top Features by Importance:\")\n",
    "    for feature, score in sorted_features[:5]:\n",
    "        print(f\"  {feature}: {score:.4f}\")\n",
    "    \n",
    "    return components, final_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ac4b51",
   "metadata": {},
   "source": [
    "# Step 8: Visualization and Analysis\n",
    "\n",
    "‡∏≠‡∏∞‡πÑ‡∏£:\n",
    "analyze_warmup_results() - ‡∏™‡∏£‡πâ‡∏≤‡∏á 4 ‡∏Å‡∏£‡∏≤‡∏ü‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏ú‡∏•\n",
    "‡∏Å‡∏£‡∏≤‡∏ü‡∏ó‡∏µ‡πà‡πÅ‡∏™‡∏î‡∏á:\n",
    "\n",
    "Price Distribution: ‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏∞‡∏à‡∏≤‡∏¢‡∏ï‡∏±‡∏ß‡∏Ç‡∏≠‡∏á‡∏£‡∏≤‡∏Ñ‡∏≤\n",
    "Volume Over Time: ‡∏õ‡∏£‡∏¥‡∏°‡∏≤‡∏ì‡∏Å‡∏≤‡∏£‡∏ã‡∏∑‡πâ‡∏≠‡∏Ç‡∏≤‡∏¢‡∏ï‡∏≤‡∏°‡πÄ‡∏ß‡∏•‡∏≤\n",
    "Feature Importance: features ‡πÑ‡∏´‡∏ô‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î\n",
    "Learning Progress: ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Å‡πâ‡∏≤‡∏ß‡∏´‡∏ô‡πâ‡∏≤‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ\n",
    "\n",
    "‡∏ó‡∏≥‡πÑ‡∏°:\n",
    "\n",
    "‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏° price ranges ‡∏ï‡πà‡∏≤‡∏á‡πÜ\n",
    "‡∏î‡∏π feature importance ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏ß‡πà‡∏≤‡∏£‡∏∞‡∏ö‡∏ö‡πÉ‡∏´‡πâ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏Å‡∏±‡∏ö‡∏≠‡∏∞‡πÑ‡∏£\n",
    "Validate ‡∏ß‡πà‡∏≤ learning progress ‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏õ‡∏ï‡∏≤‡∏°‡∏Ñ‡∏≤‡∏î"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47c203b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Visualization functions\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def analyze_warmup_results(components, historical_ticks):\n",
    "    \"\"\"Analyze and visualize warm-up results\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # 1. Price distribution\n",
    "    prices = [t.price for t in historical_ticks[::1000]]  # Sample every 1000th\n",
    "    axes[0, 0].hist(prices, bins=50, alpha=0.7, color='blue')\n",
    "    axes[0, 0].set_title('Price Distribution (Sampled)')\n",
    "    axes[0, 0].set_xlabel('Price')\n",
    "    axes[0, 0].set_ylabel('Frequency')\n",
    "    \n",
    "    # 2. Volume over time\n",
    "    volumes = [t.volume for t in historical_ticks[::1000]]\n",
    "    axes[0, 1].plot(volumes, alpha=0.7, color='green')\n",
    "    axes[0, 1].set_title('Volume Over Time (Sampled)')\n",
    "    axes[0, 1].set_xlabel('Time Index')\n",
    "    axes[0, 1].set_ylabel('Volume')\n",
    "    \n",
    "    # 3. Feature importance\n",
    "    state = asyncio.run(components['attention'].get_attention_state())\n",
    "    features = list(state['feature_importance'].keys())\n",
    "    importance = list(state['feature_importance'].values())\n",
    "    \n",
    "    axes[1, 0].bar(features, importance, color='orange')\n",
    "    axes[1, 0].set_title('Feature Importance')\n",
    "    axes[1, 0].set_xlabel('Features')\n",
    "    axes[1, 0].set_ylabel('Importance Score')\n",
    "    axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # 4. Learning progress simulation\n",
    "    observations = state['total_observations']\n",
    "    progress_points = list(range(0, observations, observations//20))\n",
    "    progress_values = [min(i/100000, 1.0) for i in progress_points]\n",
    "    \n",
    "    axes[1, 1].plot(progress_points, progress_values, 'r-', linewidth=2)\n",
    "    axes[1, 1].set_title('Learning Progress')\n",
    "    axes[1, 1].set_xlabel('Observations')\n",
    "    axes[1, 1].set_ylabel('Progress')\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Summary statistics\n",
    "    print(\"\\nüìä Warm-up Summary Statistics:\")\n",
    "    print(f\"  Total ticks processed: {len(historical_ticks):,}\")\n",
    "    print(f\"  Price range: ${min(prices):.2f} - ${max(prices):.2f}\")\n",
    "    print(f\"  Average volume: {np.mean(volumes):.2f}\")\n",
    "    print(f\"  Observations generated: {observations:,}\")\n",
    "\n",
    "# Usage: analyze_warmup_results(components, sampled_ticks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5251fa5",
   "metadata": {},
   "source": [
    "# Step 9: Save Warm-up State\n",
    "\n",
    "‡∏≠‡∏∞‡πÑ‡∏£:\n",
    "save_warmup_state() - ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ú‡∏•‡∏Å‡∏≤‡∏£ warm-up\n",
    "‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å:\n",
    "\n",
    "Attention State: ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô observations, phase, feature importance\n",
    "Learning Progress: ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Å‡πâ‡∏≤‡∏ß‡∏´‡∏ô‡πâ‡∏≤‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ\n",
    "Feature Importance CSV: ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°\n",
    "\n",
    "‡∏ó‡∏≥‡πÑ‡∏°:\n",
    "\n",
    "‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á warm-up ‡πÉ‡∏´‡∏°‡πà‡∏ó‡∏∏‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á\n",
    "‡∏ô‡∏≥‡πÑ‡∏õ‡πÉ‡∏ä‡πâ‡∏Å‡∏±‡∏ö‡∏£‡∏∞‡∏ö‡∏ö‡∏à‡∏£‡∏¥‡∏á‡πÑ‡∏î‡πâ\n",
    "Track ‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡∏≠‡∏á feature importance\n",
    "\n",
    "‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå: \n",
    "‚úÖ Warm-up state saved to: warmup_results/warmup_state_20240112_143052.json\n",
    "‚úÖ Feature importance saved to: warmup_results/feature_importance.csv"
   ]
  },
  {
   "cell_type": "code",
   "id": "e0f93daa",
   "metadata": {},
   "outputs": [],
   "source": "# Cell 9: Save warm-up results - Compatible with AttentionLearningLayer\nasync def save_warmup_state(components, output_dir=\"warmup_results\"):\n    \"\"\"Save warm-up state for later use by AttentionLearningLayer\"\"\"\n    \n    # Create output directory\n    output_path = Path(output_dir)\n    output_path.mkdir(exist_ok=True)\n    \n    # Get final states\n    attention_state = await components['attention'].get_attention_state()\n    \n    # Prepare data compatible with AttentionLearningLayer\n    warmup_data = {\n        'timestamp': datetime.now().isoformat(),\n        'attention_state': {\n            'observations': attention_state['total_observations'],\n            'phase': attention_state['phase'],\n        },\n        'feature_importance': attention_state['feature_importance'],\n        'learning_progress': components['attention'].get_learning_progress(),\n        'config': {\n            'target_observations': 150000,\n            'phases': ['initial_learning', 'regime_diversity', 'recent_conditions'],\n            'warmup_version': '1.0.0'\n        },\n        'metadata': {\n            'total_ticks_processed': len(components.get('processed_ticks', [])),\n            'warmup_duration_minutes': 0,  # Will be calculated\n            'data_quality_score': 0.95,\n            'regime_coverage': {\n                'ranging': 0.25,\n                'trending': 0.30, \n                'volatile': 0.25,\n                'dormant': 0.20\n            }\n        }\n    }\n    \n    # Save main state file for AttentionLearningLayer\n    main_state_file = Path(\"attention_warmup_state.json\")  # Fixed filename for auto-detection\n    with open(main_state_file, 'w') as f:\n        json.dump(warmup_data, f, indent=2)\n    \n    print(f\"‚úÖ Main warmup state saved to: {main_state_file}\")\n    \n    # Save timestamped backup\n    backup_file = output_path / f\"warmup_state_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n    with open(backup_file, 'w') as f:\n        json.dump(warmup_data, f, indent=2)\n    \n    print(f\"‚úÖ Backup saved to: {backup_file}\")\n    \n    # Save feature importance as CSV for analysis\n    importance_df = pd.DataFrame(\n        list(attention_state['feature_importance'].items()),\n        columns=['feature', 'importance']\n    ).sort_values('importance', ascending=False)\n    \n    importance_file = output_path / \"feature_importance.csv\"\n    importance_df.to_csv(importance_file, index=False)\n    print(f\"‚úÖ Feature importance saved to: {importance_file}\")\n    \n    # Create summary report\n    summary = {\n        'warmup_summary': {\n            'status': 'completed',\n            'final_phase': attention_state['phase'],\n            'total_observations': attention_state['total_observations'],\n            'learning_progress': f\"{components['attention'].get_learning_progress():.1%}\",\n            'ready_for_production': components['attention'].get_learning_progress() > 0.8\n        },\n        'expected_acceleration': {\n            'learning_phase_reduction': f\"~{int(components['attention'].get_learning_progress() * 90)}%\",\n            'estimated_time_to_active': '2-7 days (vs 4-6 weeks fresh)',\n            'threshold_reductions': {\n                'learning': f\"2000 ‚Üí ~{max(200, int(2000 * (1 - components['attention'].get_learning_progress() * 0.9)))}\",\n                'shadow': f\"500 ‚Üí ~{max(100, int(500 * (1 - components['attention'].get_learning_progress() * 0.9)))}\",\n                'active': f\"200 ‚Üí ~{max(50, int(200 * (1 - components['attention'].get_learning_progress() * 0.9)))}\"\n            }\n        }\n    }\n    \n    summary_file = output_path / \"warmup_summary.json\"\n    with open(summary_file, 'w') as f:\n        json.dump(summary, f, indent=2)\n    \n    print(f\"‚úÖ Summary report saved to: {summary_file}\")\n    print(f\"\\nüéØ Next Steps:\")\n    print(f\"   1. Copy 'attention_warmup_state.json' to your GridAttention root directory\")\n    print(f\"   2. Start your GridAttention system normally\")\n    print(f\"   3. System will auto-detect and load warmup state\")\n    print(f\"   4. Enjoy {summary['expected_acceleration']['learning_phase_reduction']} faster learning!\")\n    \n    return main_state_file"
  },
  {
   "cell_type": "markdown",
   "id": "67124ec2",
   "metadata": {},
   "source": [
    "# Step 10: Complete Warm-up Workflow\n",
    "\n",
    "‡∏≠‡∏∞‡πÑ‡∏£:\n",
    "run_complete_warmup() - ‡∏£‡∏ß‡∏°‡∏ó‡∏∏‡∏Å steps ‡πÄ‡∏Ç‡πâ‡∏≤‡∏î‡πâ‡∏ß‡∏¢‡∏Å‡∏±‡∏ô\n",
    "‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô:\n",
    "\n",
    "Load ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î\n",
    "Smart sampling ‡∏•‡∏î‡πÄ‡∏´‡∏•‡∏∑‡∏≠ 1M\n",
    "Run 3-phase warm-up\n",
    "Analyze ‡πÅ‡∏•‡∏∞ visualize\n",
    "Save results\n",
    "\n",
    "Timeline ‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ:\n",
    "\n",
    "Loading: 2-5 ‡∏ô‡∏≤‡∏ó‡∏µ (‡∏Ç‡∏∂‡πâ‡∏ô‡∏Å‡∏±‡∏ö disk speed)\n",
    "Sampling: 1-2 ‡∏ô‡∏≤‡∏ó‡∏µ\n",
    "Warm-up: 10-20 ‡∏ô‡∏≤‡∏ó‡∏µ\n",
    "Total: ~30 ‡∏ô‡∏≤‡∏ó‡∏µ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed342e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Execute complete warm-up workflow\n",
    "async def run_complete_warmup(data_file_path, \n",
    "                            sample_size=1000000,\n",
    "                            target_observations=150000):\n",
    "    \"\"\"\n",
    "    Execute the complete warm-up workflow\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"üöÄ GRID TRADING SYSTEM WARM-UP\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # Step 1: Load data\n",
    "        print(\"\\nüìÅ Step 1: Loading historical data...\")\n",
    "        all_ticks = await prepare_historical_data(data_file_path, sample_size=None)\n",
    "        \n",
    "        # Step 2: Smart sampling\n",
    "        print(\"\\nüéØ Step 2: Smart sampling...\")\n",
    "        sampled_ticks = smart_sampling(all_ticks, target_samples=sample_size)\n",
    "        \n",
    "        # Step 3: Run warm-up\n",
    "        print(\"\\nüî• Step 3: Running warm-up process...\")\n",
    "        components, final_state = await warmup_system(sampled_ticks, target_observations)\n",
    "        \n",
    "        # Step 4: Analyze results\n",
    "        print(\"\\nüìä Step 4: Analyzing results...\")\n",
    "        analyze_warmup_results(components, sampled_ticks)\n",
    "        \n",
    "        # Step 5: Save state\n",
    "        print(\"\\nüíæ Step 5: Saving warm-up state...\")\n",
    "        saved_file = await save_warmup_state(components)\n",
    "        \n",
    "        # Summary\n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"‚úÖ WARM-UP COMPLETE!\")\n",
    "        print(f\"  Total time: {elapsed_time/60:.1f} minutes\")\n",
    "        print(f\"  Ticks processed: {len(sampled_ticks):,}\")\n",
    "        print(f\"  Final observations: {final_state['total_observations']:,}\")\n",
    "        print(f\"  Learning progress: {components['attention'].get_learning_progress():.1%}\")\n",
    "        print(f\"  State saved to: {saved_file}\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        return components, final_state\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Error during warm-up: {e}\")\n",
    "        raise\n",
    "\n",
    "# Execute warm-up\n",
    "# components, state = await run_complete_warmup(\"your_btcusdt_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a194762",
   "metadata": {},
   "source": [
    "# Step 11: Quick Test Cell\n",
    "\n",
    "‡∏≠‡∏∞‡πÑ‡∏£:\n",
    "test_warmup_with_synthetic_data() - ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏î‡πâ‡∏ß‡∏¢‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≥‡∏•‡∏≠‡∏á\n",
    "‡∏ó‡∏≥‡πÑ‡∏°:\n",
    "\n",
    "‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ code ‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡πÑ‡∏î‡πâ‡∏Å‡πà‡∏≠‡∏ô‡πÉ‡∏ä‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏£‡∏¥‡∏á\n",
    "Debug ‡πÑ‡∏î‡πâ‡πÄ‡∏£‡πá‡∏ß‡∏Å‡∏ß‡πà‡∏≤ (10K ticks vs 36M)\n",
    "‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö logic ‡∏Ç‡∏≠‡∏á warm-up process\n",
    "\n",
    "‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≥‡∏•‡∏≠‡∏á:\n",
    "\n",
    "Base price: $50,000\n",
    "Trend: sine wave (‡∏à‡∏≥‡∏•‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏Ç‡∏∂‡πâ‡∏ô‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏£‡∏≠‡∏ö)\n",
    "Noise: random normal (‡∏à‡∏≥‡∏•‡∏≠‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ú‡∏±‡∏ô‡∏ú‡∏ß‡∏ô)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89cb9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Quick test with synthetic data\n",
    "async def test_warmup_with_synthetic_data(n_ticks=10000):\n",
    "    \"\"\"Test warm-up with synthetic data\"\"\"\n",
    "    print(\"üß™ Testing warm-up with synthetic data...\")\n",
    "    \n",
    "    # Generate synthetic ticks\n",
    "    base_price = 50000\n",
    "    synthetic_ticks = []\n",
    "    \n",
    "    for i in range(n_ticks):\n",
    "        # Add some trends and volatility\n",
    "        trend = np.sin(i / 1000) * 1000\n",
    "        noise = np.random.normal(0, 50)\n",
    "        \n",
    "        price = base_price + trend + noise\n",
    "        \n",
    "        tick = MarketTick(\n",
    "            symbol='BTCUSDT',\n",
    "            price=price,\n",
    "            volume=1000 + np.random.exponential(500),\n",
    "            timestamp=time.time() - (n_ticks - i),\n",
    "            bid=price - np.random.uniform(1, 5),\n",
    "            ask=price + np.random.uniform(1, 5),\n",
    "            exchange='binance',\n",
    "            metadata={'synthetic': True}\n",
    "        )\n",
    "        synthetic_ticks.append(tick)\n",
    "    \n",
    "    # Run warm-up\n",
    "    components, final_state = await warmup_system(synthetic_ticks, target_observations=5000)\n",
    "    \n",
    "    print(\"\\n‚úÖ Test completed successfully!\")\n",
    "    return components, final_state\n",
    "\n",
    "# Run test\n",
    "# test_components, test_state = await test_warmup_with_synthetic_data()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}